data_configs:
  lang_pair: "en-de"
  train_data:
  - "/home/public_data/nmtdata/wmt14_en-de_data_selection/train.tok.clean.en"
  - "/home/public_data/nmtdata/wmt14_en-de_data_selection/train.tok.clean.de"
  valid_data:
  - "/home/public_data/nmtdata/wmt14_en-de_data_selection/newstest2013.tok.en"
  - "/home/public_data/nmtdata/wmt14_en-de_data_selection/newstest2013.tok.de"
  bleu_valid_reference: "/home/public_data/nmtdata/wmt14_en-de_data_selection/newstest2013.de"
  vocabularies:
  - type: "bpe"
    dict_path: "/home/public_data/nmtdata/wmt14_en-de_data_selection/vocab.json"
    max_n_words: -1
    codes: "/home/public_data/nmtdata/wmt14_en-de_data_selection/bpe.32000"
  - type: "bpe"
    dict_path: "/home/public_data/nmtdata/wmt14_en-de_data_selection/vocab.json"
    max_n_words: -1
    codes: "/home/public_data/nmtdata/wmt14_en-de_data_selection/bpe.32000"
  max_len:
  - 100
  - 100
  num_refs: 1

model_configs:
  model: DL4MT
  d_word_vec: 512
  d_model: 1024
  dropout: 0.5
  proj_share_weight: true
  bridge_type: "mlp"

optimizer_configs:
  optimizer: "adam"
  learning_rate: 0.0005
  grad_clip: 1.0
  optimizer_params: ~
  schedule_method: loss
  scheduler_configs:
    min_lr: 0.00005
    scale: 0.5
    patience: 2

training_configs:
  seed: 1234
  max_epochs: 50
  shuffle: true
  use_bucket: true
  batch_size: 2000
  batching_key: "tokens"
  update_cycle: 10
  valid_batch_size: 20
  disp_freq: 100
  save_freq: 500
  num_kept_checkpoints: 1
  loss_valid_freq: 500
  bleu_valid_freq: 500
  bleu_valid_batch_size: 5
  bleu_valid_warmup: 0
  bleu_valid_configs:
    max_steps: 150
    beam_size: 4
    alpha: 0.6
    sacrebleu_args: "--tokenize intl -lc"
    postprocess: true
  early_stop_patience: 50
