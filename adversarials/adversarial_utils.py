import osimport torchimport yamlimport torch.nn as nnfrom collections import OrderedDictimport numpy as npfrom src.data.vocabulary import Vocabularyfrom src.data.vocabulary import PAD, BOS, EOS, UNKimport subprocessimport timemodels_path = ["/home/public_data/nmtdata/nmt-baselines/transformer-wmt15-enfr/small_baseline/baseline_enfr_save/transformer.best.final",               "/home/zouw/NJUNMT-pytorch/scripts/save_zhen_bpe/transformer.best.final"]configs_path = ["/home/public_data/nmtdata/nmt-baselines/transformer-wmt15-enfr/small_baseline/transformer_wmt15_en2fr.yaml",                "/home/zouw/NJUNMT-pytorch/configs/transformer_nist_zh2en_bpe.yaml"]def random_text_selection(config_path,                          data_size=100,                          save_log="random_sents"):    # load configs    np.random.seed(32767)    with open(config_path.strip()) as f:        configs = yaml.load(f)    data_configs = configs["data_configs"]    return_set = []    with open(data_configs["train_data"][0], "r") as src, open(save_log, "w") as out:        i = 0        for line in src:            if np.random.uniform() > 0.5 and i < data_size:                i += 1                out.write(line)                return_set += [line]    return return_setdef random_pair_selection(config_path,                          data_size=100,                          save_log="random_sents"):    """    randomly choose from parallel data, and save to the save_logs    :param config_path:    :param data_size:    :param save_log:    :return: random selected pairs    """    np.random.seed(32767)    with open(config_path.strip()) as f:        configs = yaml.load(f)    data_configs = configs["data_configs"]    with open(data_configs["train_data"][0], "r") as src, \        open(data_configs["train_data"][1], "r") as trg, \        open(save_log+".src", "w") as out_src, open(save_log+".trg", "w") as out_trg:        counter=0        return_src=[]        return_trg=[]        for sent_s, sent_t in zip(src,trg):            if np.random.uniform()<0.2 and counter<data_size:                counter += 1                out_src.write(sent_s)                out_trg.write(sent_t)                return_src+=[sent_s.strip()]                return_trg+=[sent_t.strip()]    return return_src, return_trgdef load_or_extract_near_vocab(config_path,                               model_path,                               save_to,                               save_to_full,                               init_perturb_rate=1.0,                               batch_size=50,                               top_reserve=12,                               reload=True,                               emit_as_id=False):    """based on the embedding parameter from Encoder, extract near vocabulary for all words    return: dictionary of vocabulary of near vocabs; and a the saved file    :param config_path: (string) victim configs (for training data and vocabulary)    :param model_path: (string) victim model path for trained embeddings    :param save_to: (string) directory to store distilled near-vocab    :param save_to_full: (string) directory to store full near-vocab    :param init_perturb_rate: (float) the weight-adjustment for perturb    :param batch_size: (integer) extract near vocab by batched cosine/Euclidean-similarity    :param top_reserve: (integer) at most reserve top-k near candidates    :param reload: reload from the save_to_path if previous record exists    :param emit_as_id: (boolean) the key in return will be token ids instead of token    """    # load configs    with open(config_path.strip()) as f:        configs = yaml.load(f)    data_configs = configs["data_configs"]    model_configs = configs["model_configs"]    # load vocabulary file    src_vocab = Vocabulary(**data_configs["vocabularies"][0])    # load embedding from model    emb = nn.Embedding(num_embeddings=src_vocab.max_n_words,                       embedding_dim=model_configs["d_word_vec"],                       padding_idx=PAD                       )    model_params = torch.load(model_path, map_location="cpu")    emb.load_state_dict({"weight": model_params["encoder.embeddings.embeddings.weight"]},                        strict=True)    len_mat = torch.sum(emb.weight**2, dim=1)**0.5  # length of the embeddings    if os.path.exists(save_to) and reload:        print("load from %s:" % save_to)        return load_perturb_weight(save_to, src_vocab, emit_as_id)    else:        print("collect near candidates for vocabulary")        avg_dist = 0        avg_std = []        counter = 0        word2p = OrderedDict()        word2near_vocab = OrderedDict()        # omit similar vocabulary file (batched)        with open(save_to, "w") as similar_vocab, open(save_to_full, "w") as full_similar_vocab:            # every batched vocabulary collect average E-dist            for i in range((src_vocab.max_n_words//batch_size)+1):                index = torch.tensor(range(i*batch_size,                              min(src_vocab.max_n_words, (i+1)*batch_size),                              1))                # extract embedding data                slice_emb = emb(index)                collect_len = torch.mm(len_mat.narrow(0, i * batch_size, min(src_vocab.max_n_words, (i+1)*batch_size)-i*batch_size).unsqueeze(1),                                len_mat.unsqueeze(0))                # filter top 10 nearest vocab, then filter with Eul-distance within certain range                similarity = torch.mm(slice_emb,                                       emb.weight.t()).div(collect_len)                # get value and index                topk_index = similarity.topk(top_reserve, dim=1)[1]                sliceemb = slice_emb.unsqueeze(dim=1).repeat(1, top_reserve, 1)  # [batch_size, 1*8, dim]                E_dist = ((emb(topk_index)-sliceemb)**2).sum(dim=-1)**0.5                # print("avg Euclidean distance:", E_dist)                avg_dist += E_dist.mean()                avg_std += [E_dist.std(dim=1).mean()]                counter += 1            avg_dist = avg_dist.item() / counter            # print(avg_dist)  # tensor object            # print(avg_std)            # output to file and return dictionary            for i in range((src_vocab.max_n_words//batch_size)+1):                index = torch.tensor(range(i*batch_size,                              min(src_vocab.max_n_words, (i+1)*batch_size),                              1))                # extract embedding data                slice_emb = emb(index)                collect_len = torch.mm(len_mat.narrow(0, i * batch_size, min(src_vocab.max_n_words, (i+1)*batch_size)-i*batch_size).unsqueeze(1),                                       len_mat.unsqueeze(0))                # filter top k nearest vocab with cosine-similarity                similarity = torch.mm(slice_emb,                                      emb.weight.t()).div(collect_len)                topk_val, topk_indices = similarity.topk(top_reserve, dim=1)                # calculate E-dist                sliceemb = slice_emb.unsqueeze(dim=1).repeat(1, top_reserve, 1)  # [batch_size, 1*topk, dim]                E_dist = ((emb(topk_indices)-sliceemb)**2).sum(dim=-1)**0.5                topk_val = E_dist.cpu().detach().numpy()                topk_indices = topk_indices.cpu().detach().numpy()                for j in range(topk_val.shape[0]):                    bingo = 0.                    src_word_id = j + i*batch_size                    src_word = src_vocab.id2token(src_word_id)                    near_vocab = []                    # if emit_as_id:                    #     near_vocab = [src_word_id]                    # else:                    #     near_vocab = [src_word]                    similar_vocab.write(src_word + "\t")                    full_similar_vocab.write(src_word + "\t")                    for k in range(1, topk_val.shape[1]):                        near_cand_id = topk_indices[j][k]                        near_cand = src_vocab.id2token(near_cand_id)                        full_similar_vocab.write(near_cand + "\t")                        if topk_val[j][k] < avg_dist and (near_cand_id not in [PAD, EOS, BOS]):                            bingo += 1                            similar_vocab.write(near_cand + "\t")                            if emit_as_id:                                near_vocab += [near_cand_id]                            else:                                near_vocab += [near_cand]                        if k == topk_val.shape[1]-1 and bingo == 0:                            # to make life easier, we keep at least one candidate and UNK                            last_cand_ids =[UNK, topk_indices[j][1]]                            for final_reserve_id in last_cand_ids:                                last_cand = src_vocab.id2token(final_reserve_id)                                similar_vocab.write(last_cand + "\t")                                if emit_as_id:                                    near_vocab += [final_reserve_id]                                else:                                    near_vocab += [last_cand]                    probability = init_perturb_rate * bingo/(len(src_word)*top_reserve)                    similar_vocab.write("\t"+str(probability)+"\n")                    full_similar_vocab.write("\t"+str(probability)+"\n")                    if emit_as_id:                        word2near_vocab[src_word_id] = near_vocab                        word2p[src_word_id] = probability                    else:                        word2near_vocab[src_word] = near_vocab                        word2p[src_word] = probability        return word2p, word2near_vocab# load the probability of randomization from existing filesdef load_perturb_weight(save_to, src_vocab=None, emit_as_id=False):    """    random probability for the words.    :param save_to: (string) saved files indicating top k most vocab    :param src_vocab: Vocabulary class object (emit dictionary using token id)    :param emit_as_id: (boolean) whether the dictionary use token ids    :return: two dicts contains the random probability of dict <token: probability>              <token: <candidate: probability>>    """    if emit_as_id:        assert src_vocab is not None, "src_vocab must be provided when emit_as_id"    with open(save_to) as similar_vocab:        word2p = OrderedDict()        word2near_vocab = OrderedDict()        for line in similar_vocab:            line = line.split("\t")            if emit_as_id:                word2near_vocab[src_vocab.token2id(line[0])] = [src_vocab.token2id(i) for i in line[1:-2]]                word2p[src_vocab.token2id(line[0])] = float(line[-1])            else:                word2near_vocab[line[0]] = line[1:-2]                word2p[line[0]] = float(line[-1])    return word2p, word2near_vocab# load translation model parametersdef load_translate_model(path, map_location="cpu"):    state_dict = torch.load(path, map_location=map_location)    if "model" in state_dict:        return state_dict["model"]    return state_dictdef initial_random_perturb(config_path,                           inputs,                           w2p, w2vocab,                           mode="len_based",                           key_type="token",                           show_bleu=False):    """    batched random perturb, perturb is based on random probability from the collected candidates    meant to test initial attack rate.    :param config_path: victim configs    :param inputs: raw batched input (list) sequences in [batch_size, seq_len]    :param w2p: indicates how likely a word is perturbed    :param w2vocab: near candidates    :param mode: based on word2near_vocab, how to distribute likelihood among candidates    :param key_type: inputs are given by raw sequences of tokens or tokenized labels    :param show_bleu: whether to show bleu of perturbed seqs (compare to original seqs)    :return: list of perturbed inputs and list of perturbed flags    """    np.random.seed(int(time.time()))    assert mode in ["uniform", "len_based"], "Mode must be in uniform or multinomial."    assert key_type in ["token", "label"], "inputs key type must be token or label."    # load configs    with open(config_path.strip()) as f:        configs = yaml.load(f)    data_configs = configs["data_configs"]    # load vocabulary file and tokenize    src_vocab = Vocabulary(**data_configs["vocabularies"][0])    perturbed_results = []    flags = []    for sent in inputs:        if np.random.uniform() < 0.5:  # perturb            perturbed_sent = []            if key_type == "token":                tokenized_sent = src_vocab.tokenizer.tokenize(sent)                for word in tokenized_sent:                    if np.random.uniform() < w2p[word]:                        # need to perturb on lexical level                        if mode == "uniform":                            # uniform choose from candidates:                            perturbed_sent += [w2vocab[word][np.random.choice(len(w2vocab[word]),                                                                              1)[0]]]                        elif mode == "len_based":                            # weighted choose from candidates:                            weights = [1./(1+abs(len(word)-len(c))) for c in w2vocab[word]]                            norm_weights = [c/sum(weights) for c in weights]                            perturbed_sent += [w2vocab[word][np.random.choice(len(w2vocab[word]),                                                                              1,                                                                              p=norm_weights                                                                              )[0]]]                    else:                        perturbed_sent += [word]                # print(perturbed_sent)  # yield same form of sequences of tokens                perturbed_sent = src_vocab.tokenizer.detokenize(perturbed_sent)            elif key_type == "label":  # tokenized labels                for word_index in sent:                    word = src_vocab.id2token(word_index)                    if np.random.uniform() < w2p[word]:                        if mode == "uniform":                            # uniform choose from candidates:                            perturbed_label = src_vocab.token2id(w2vocab[word][np.random.choice(                                len(w2vocab[word]), 1                            )[0]])                            perturbed_sent += [perturbed_label]                        elif mode == "len_based":                            # weighted choose from candidates:                            weights = [1. / (1 + abs(len(word) - len(c))) for c in w2vocab[word]]                            norm_weights = [c / sum(weights) for c in weights]                            perturbed_label = src_vocab.token2id(w2vocab[word][np.random.choice(len(w2vocab[word]),                                                                             1,                                                                             p=norm_weights                                                                            )[0]])                            perturbed_sent += [perturbed_label]                    else:                        perturbed_sent += [word_index]            perturbed_results += [perturbed_sent]            flags += [1]            # out.write(perturbed_sent + "\n")        else:            perturbed_results += [sent]            flags += [0]    return perturbed_results, flagsdef corpus_bleu_char(hyp_in, ref_in, need_tokenized=True):    """    preprocess corpus into char level and test BLEU,    proposed to check modification rate    :param hyp_in: files to be tested    :param ref_in: reference file    :param need_tokenized: for languages needs tokenization    :return:    """    with open(hyp_in, "r") as hyp, open(ref_in, "r") as ref, \            open("hyp_char", "w") as hyp_char, open("ref_char", "w") as ref_char:        for line_hyp_in, line_ref_in in zip(hyp, ref):            if not need_tokenized:                line_hyp_in = line_hyp_in.replace(" ", "")                line_ref_in = line_ref_in.replace(" ", "")            hyp_char.write(" ".join(list(line_hyp_in)))            ref_char.write(" ".join(list(line_ref_in)))    # cat hyp_char | sacrebleu -lc --score-only  ref_char    # sacrebleu_cmd = ["sacrebleu", "-l"] + ["--score-only",]+["ref_char"]    cat = subprocess.Popen(("cat", "hyp_char"), stdout=subprocess.PIPE)    cmd_bleu = subprocess.Popen(("/home/zouw/anaconda3/bin/sacrebleu", "-lc", "--score-only", "--force", "ref_char"),                                stdin=cat.stdout,                                stdout=subprocess.PIPE)    bleu = cmd_bleu.communicate()[0].decode("utf-8").strip()    print(bleu)    bleu = float(bleu)    subprocess.Popen("rm ref_char hyp_char", shell=True)    return bleu# random_chosed_inputs = random_pair_selection(config_path=configs_path[1],#                                              data_size=100)[0]# w2p, w2vocab = load_or_extract_near_vocab(config_path=configs_path[1],#                                           model_path=models_path[1],#                                           save_to="similar_vocab",#                                           save_to_full="full_similar_vocab",#                                           top_reserve=12)# purturbed_seqs = initial_random_perturb(config_path=configs_path[1],#                                         inputs=random_chosed_inputs,#                                         w2p=w2p, w2vocab=w2vocab,#                                         mode="len_based",#                                         key_type="token",#                                         show_bleu=False)[0]# print(purturbed_seqs)# with open("perturbed_sents","w") as f:#     for sent in purturbed_seqs:#         f.write(sent)#         f.write("\n")## corpus_bleu_char("perturbed_sents", "random_sents.src")